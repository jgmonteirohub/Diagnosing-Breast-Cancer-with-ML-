---
title: "Wisconsin Breast Cancer Data"
author: "Jonathan Monteiro"
date: "11/30/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=FALSE)
```

```{r, message=FALSE,warning=FALSE}
#Load libraries and data from csv file

library(GGally)
library(ggplot2)
library(caret)
library(plyr)
library(ROCR)
library(pander)
library(dplyr)
library('ggpubr')
library("naniar")
library('verification')

# Read the dataset and store it within a variable

cancerfile <- read.csv("data.csv")

# Convert the outcome variable into a factor variable

cancerfile$diagnosis <- as.factor(cancerfile$diagnosis)

# Remove unhelpful columns prior to analysis

cancerfile$X <- NULL
cancerfile$id <- NULL
```

# Introduction 

The goal of this project was to create a model that could accurately diagnose a possibly cancerous breast mass, as well as see which variables were most significant within such a model. The model would aim to classify breast masses as either "Benign" or "Malignant." "Benign" tumors are tumors that are non-cancerous, slow growing and non-invasive. Benign tumors are typically smaller and have a well-defined shape. "Malignant" tumors are tumors that are cancerous, fast growing and spread to other parts of the body. They are typically larger, and have an uneven or irregular shape. These physical characteristics assist doctors in diagnosing tumors correctly, and would potentially help a statistical model as well. 

Breast cancer is one of the most devastating diseases that we currently face in modern day society. Women are especially the victims of this disease. Breast cancer is the most common cancer in American women, only behind skin cancers. 1 in 8 women in the United States will develop breast cancer in their lifetime. On average, every 2 minutes a woman is diagnosed with breast cancer in the US. Unfortunate facts like these can serve as motivation to create new approaches in effectively diagnosing breast masses as either benign or malignant. Many of the approaches that can be used revolve around machine learning and statistical models.

# The Data

For this project, I was given the "Wisconsin Breast Cancer" dataset. The data focuses on digitized images of a fine needle aspirate of a breast mass and was collected by the University of California Irvine. In total, there were 569 observations of breast masses. Additionally, the dataset contained ten physical characteristics of each breast mass. These ten physical characteristics were the mass' radius, texture, perimeter, area, smoothness, compactness, concavity, concave points, and symmetry. For each of these characteristics, three features were computed. The features included the characteristics' mean, standard error and the mean of the three largest values of that tumor. For ten characteristics with three features each, this led to 30 total independent variables. 


```{r}
# Preview the first six rows of the dataset

head(cancerfile[,1:5]) %>% pander
```

<center>
Figure 1: A preview of the "Wisconsin Breast Cancer" dataset

 
```{r}
# Summarize each of the variables with descriptive statistics

summary(cancerfile) 
```


<center>
Figure 2: Descriptive statistics for each of the variables within the dataset



## Cleaning and Transforming Data 

Prior to analysis, it is necessary to inspect the data to find abnormalities, missing values, unnecessary columns etc. Any such instances can negatively effect the model's predictions and overall performance. 

```{r, echo=FALSE, results='hide'}

# A function from the 'naniar' library that can detect whether or not missing data is present within the entire datset.

any_na(cancerfile)

# The function returns 'FALSE', meaning no instances of missing data were detected
```

No instances of missing data were located within the dataset. The "id" and the "X" columns were removed as neither columns would provide any insight. Lastly, the dependent variable was transformed into a factor variable prior to analysis. 

# Exploratory Data Analysis


Exploratory data analysis is a crucial method that can assist with observing the characteristics of a dataset, discovering its patterns, and identifying potential relationships. Several visualizations were utilized in order to better understand the dataset prior to the model building process.


<center>
```{r}
ggplot(cancerfile, aes(diagnosis))+
  geom_bar(fill = "#0073C2FF") + 
  labs(
        x = "Diagnosis",
        y = "Count",
        title = "Frequency of Each Diagnosis")
```
<center>
Figure 3: Frequency of benign and malignant breast masses in dataset


Of the 569 breast masses within the data, 357 are 'benign' and 212 are 'malignant'. This means 62.7% of the breast masses are benign while the remaining 37.3% are malignant.  

As mentioned previously, one of the key differences between benign and malignant skin masses is their size. The below visualization supports this point, as increases in the radius mean and concavity mean lead to more cases of malignant skin masses. 


```{r}
ggplot(cancerfile, aes(x = radius_mean, y = concavity_mean)) +
    geom_point(aes(color = diagnosis)) +
  
  labs(
        x = "Radius Mean",
        y = "Concavity Mean",
        color = "Diagnosis",
        title = "Relation between Radius Mean and Concavity Mean",
        subtitle = "Separated by cancer diagnosis"
            )
```

<center>
Figure 4: Relation between radius mean and concavity mean; separated by cancer diagnosis


This pattern appears consistent when observing the other 'mean' variables and their relationship with the diagnosis variable. 

<center>
```{r, message=FALSE}
ggpairs(cancerfile[,c(2:11,1)], aes(color=diagnosis))
```


Figure 3: A plot showcasing correlation of the "means" section of dataset; separated by diagnosis


## Analyzing Correlations 

In order to create the best possible model, the independent variables were observed in order to locate instances of multicollinearity. This refers to high correlation amongst the independent variables to one another. Multicollinearity can lead to several issues such as over complexity and poor results. Ideally, a simpler model is preferred in order to avoid such issues. The previous figure was a beneficial plot for discovering such instances. As such, the figure was used for the 'standard error' and 'worst' sections of the dataset as well. The figures can be seen below.

```{r, message=FALSE}
ggpairs(cancerfile[,c(12:21,1)], aes(color=diagnosis))
```

<center>
Figure 4: A plot showcasing correlation of the "standard error" section of dataset


```{r, message=FALSE}
ggpairs(cancerfile[,c(22:31,1)], aes(color=diagnosis))
```

<center>
Figure 4: A plot showcasing correlation of the "worst" section of dataset

# Variable Removal

By quickly examining the plot, several examples of multicollinearity can be observed. For example, between the radius, area and perimeter variables. These variable consistently showcase high correlation among one another, which is understandable considering how radius, perimeter and area are calculated. Although this was immediately noticeable by observing the figures above, visually observing the pair-wise correlations of the 30 independent variables would be time consuming. Instead, each of the pair-wise correlations were calculated and columns were chosen to be removed based upon a chosen correlation cutoff value.  The correlation cutoff was set to 0.8 and variables were removed as such. 

```{r}
# The pair-wise correlations were calculated and stored within the 'correlations' variable

correlations = cor(cancerfile[,2:31])

# Columns were then chosen to be removed based on whether or not they met the correlation cutoff value of 0.8

removedcols = findCorrelation(correlations, cutoff = 0.8, names = T)

# The chosen column names can be seen below

removedcols

# The columns were officially removed from the datset prior to analysis

cancerfile = cancerfile[, -which(colnames(cancerfile) %in% removedcols)]
```

## Partitioning the Data 

Prior to the model building process, the data was partitioned into a training and test set. A training set would be used to train the model and the testing set would be used to evaluate the model's performance.


```{r, echo=FALSE}
set.seed(43)

# Rows were selected for the training and test datasets
sample_index <- createDataPartition(cancerfile$diagnosis, times = 1, p = 0.75, list = FALSE)
train <- cancerfile[sample_index, ]
test <-  cancerfile[-sample_index, ]
```

# Model Building

As mentioned previously, we are attempting to predict whether a breast mass is malignant or benign based on its physical characteristics. This would be a classic case of a classification problem. Classification problems require specific statistical models. The statistical model we will use for this problem will be logistic regression. At first, the logistic regression model will be trained using each of the independent variables left within the dataset.

# Model #1: All Variables 

```{r, warning=FALSE}
# Fit model #1 with all of the indepedent variables present within the dataset

fullmodel <- glm(diagnosis ~ ., data = train, family="binomial")
summary(fullmodel)
```
Using a significance level of 0.05, the output suggests area_mean, radius_se, texture_se, fractal_dimension_se, texture_ worst and smoothness_worst are all statistically significant variables. For the full model, an AIC of 65.98 was calculated. This will be helpful for model comparison. Observing the null deviance and residual deviance can assist in determining the goodness of fit of a model. With this model, the null deviance of 563.81 and residual deviance of 35.98 shows a significant decrease in deviance. The model appears to explain the data very well.

Analyzing how the model performs with the test dataset is crucial to performance evaluation. The performance of classification models can be gauged through the use of an ROC plot. An ROC is a probability curve and the ROC curve is plotted with true positive rate against the false positive rate. The true positive rate refers to how many cases were correctly classified as "positive" and the false positive rate refers to how many were incorrectly classified as "positive." For the plot, the TPR is on y-axis and FPR is on the x-axis. Typically, one would want a high TPR and a low FPR. The area underneath the curve, also called the AUC, represents how well the model is able to distinguish between the classes within the problem. In this case, it refers to how well the model can classify malignant and benign breast masses. An ROC plot can be seen below. 

```{r, echo=FALSE}

# The model is used to make predictions and it's performance is compared with the true answers within the test dataset

fullprediction_lr <- predict(fullmodel, test, type="response")
fullpred <- prediction(fullprediction_lr, test$diagnosis)
fullperf <- performance(fullpred,"tpr","fpr")

# The ROC is visualized and the AUC is calculated

fullauc_ROCR <- performance(fullpred, measure = "auc")
fullauc_ROCR <- fullauc_ROCR@y.values[[1]]


plot(fullperf)
```

```{r}
print(paste("Model #1 AUC: ",round(fullauc_ROCR,4))) 
```

Model #1 has an AUC of 0.9816. This showcases exceptional predictive ability as it is very close to 1. Overall, results for model #1 are promising but the model's performance will be compared to two other models as well. 


# Model #2: Manual Selection 

The next model was more strict in the variable selection process. For this method, the output of model #1 was observed and variables that were under a significance value of 0.25 were removed from the model. This process was repeated until the model contained solely statistically significant variables. 

From observing the output of model #1, the variables area_mean, concavity_se, concave.points_se, radius_se, texture_se, fractal_dimension_se, texture_ worst and smoothness_worst can be retained after the first step in the selection process. However, after further evaluation, the concave.points_se and fractal_dimension_se variables were removed from the model as well. The final model #2 results can be seen below.
 
```{r, warning=FALSE,message=FALSE, echo=FALSE}
manualmodel <- glm(diagnosis ~ area_mean + concavity_se + radius_se + texture_se + texture_worst + smoothness_worst, data = train, family="binomial")
summary(manualmodel)
```
As you can see, all variables retained within this model are statistically significant. In the end, only 6 of the 14 variables from the full model were retained. This resulted in a simpler model with more degrees of freedom. The AIC for model #2 was 80.387, which is higher than the previous model. 

```{r, echo=FALSE}
manualprediction_lr <- predict(manualmodel, test, type="response")
manualpred <- prediction(manualprediction_lr, test$diagnosis)
manualperf <- performance(manualpred,"tpr","fpr")

manualauc_ROCR <- performance(manualpred, measure = "auc")
manualauc_ROCR <- manualauc_ROCR@y.values[[1]]

plot(manualperf)
```

```{r}
print(paste("Model #2 AUC: ",round(manualauc_ROCR,4))) 
```
When evaluating the model's performance on the test data, we can see that model #2 performs better, having an AUC of 0.9875. This is impressive given that the model is significantly simpler in terms of independent variables present within the model.

# Model #3: Forward Stepwise AIC

The last model was created by utilizing "Forward Stepwise AIC" for the variable selection process. This method started with the null model and continually added the next most significant variable to the model. The end model was ultimately determined based upon the model's AIC value.  

```{r, warning=FALSE}
# Create the null model

null <- glm(diagnosis ~ 1, family = binomial, train)

# Perform Forward Stepwise AIC selection

stepwisemodel <- step(null, scope=list(lower=null, upper=fullmodel), k = 2, direction="forward", trace=FALSE)

summary(stepwisemodel)
```

After using Forward Stepwise AIC selection, 9 of the 14 variables remained from the full model. Most of the variables within the model are statistically signicant when using a significance level of 0.05. The only variable that is not statistically significant in this case would be fractal_dimension_worst. This model has the lowest AIC of the three models, with an AIC of 59.13. Given the method chosen to select this particular model, this is understandable.

```{r, echo=FALSE}
stepprediction_lr <- predict(stepwisemodel, test, type="response")
steppred <- prediction(stepprediction_lr, test$diagnosis)
stepperf <- performance(steppred,"tpr","fpr")

stepauc_ROCR <- performance(steppred, measure = "auc")
stepauc_ROCR <- stepauc_ROCR@y.values[[1]]

plot(stepperf)
```


```{r}
print(paste("Model #3 AUC: ",round(stepauc_ROCR,4))) 
```

When evaluated using the test data, model #3 had an AUC of 0.986. Of the three models, this is the second highest calculated AUC.


# Conclusion

Overall, the results indicated that this report was an overall success. The dataset featured a robust assortment of independent variables to use in predicting the dependent variable, cancer diagnosis. This problem was a classic case of a classification problem. For that, I decided to use the most commonly used statistical model for classification problems, logistic regression. Three different models were evaluated for their overall performance. The three models were the Full Model, Maual Selection Model and the Forward Stepwise AIC mode.For evaluation, the AIC values were observed, a ROC plot was visualized and an AUC was calculated to determine the model's performance. Of the three models, the Manual Selection Model had the highest calculated AUC (0.9875) when evaluated with the test data set. This model was followed by the Forward Stepwise AIC model, (0.986) and the Full Model (0.9816). In terms of AIC, the model with the lowest AIC was the Forward Stepwise AIC model with an AIC of 59.13. This model was followed by the Full Model (65.98) and the Manual Selection model (80.387). After looking at the results, it is impressive that the Manual Selection Model was the least complex model with only 6 variables, however it was able to outperform the other models in some areas given the Full Model had 14 variables and the Forward Stepwise AIC model had 9. With this in mind, I am of the opinion that the Manual Selection Model had the most optimal performance. However, all 3 models appeared to have performed exceptionally well with this particular dataset and the end results exceeded expectations. 